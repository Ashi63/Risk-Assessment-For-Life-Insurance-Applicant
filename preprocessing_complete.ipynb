{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXESG29qybyY",
    "outputId": "4b2b3be9-1dbf-4411-ee4a-0e9806044d3f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data.!\n",
      "Data succesfully loaded.!\n",
      "\n",
      "\n",
      "Set ID as Index.!\n",
      "ID set as index succesfull.!\n",
      "\n",
      "\n",
      "Remove Duplicate rows started.!\n",
      "Duplicate Rows present in data:  30\n",
      "Data Shape With Duplicated:  (59381, 127)\n",
      "Data Shape Without Duplicated:  (59351, 127)\n",
      "No. Duplicated Rows Removed:  30\n",
      "Duplicate Rows in data:  30\n",
      "Duplicate rows removed Succesfully.!\n",
      "\n",
      "\n",
      "Spliting the Data into Independent Variable(X),Dependent Variable y started.!\n",
      "Data Splited in into Independent Variable(X),Dependent Variable y successfully.!\n",
      "\n",
      "\n",
      "Train Test Split Started.!\n",
      "Shape of X_train:  (41545, 126)\n",
      "Shape of y_train:  (41545,)\n",
      "Shape of X_test :  (17806, 126)\n",
      "Shape of y_test :  (17806,)\n",
      "Train Test Split Succesfull.!\n",
      "\n",
      "\n",
      "Missing Value details in X_train: \n",
      "                      missing_values  missing_values_%\n",
      "Medical_History_10            41172         99.102178\n",
      "Medical_History_32            40747         98.079191\n",
      "Medical_History_24            38926         93.695992\n",
      "Medical_History_15            31196         75.089662\n",
      "Family_Hist_5                 29249         70.403177\n",
      "Family_Hist_3                 24081         57.963654\n",
      "Family_Hist_2                 19921         47.950415\n",
      "Insurance_History_5           17803         42.852329\n",
      "Family_Hist_4                 13416         32.292695\n",
      "Employment_Info_6              7592         18.274161\n",
      "Medical_History_1              6271         15.094476\n",
      "Employment_Info_4              4755         11.445421\n",
      "Employment_Info_1                 9          0.021663\n",
      "\n",
      "\n",
      "Missing Value details in X_test: \n",
      "                      missing_values  missing_values_%\n",
      "Medical_History_10            17622         98.966640\n",
      "Medical_History_32            17497         98.264630\n",
      "Medical_History_24            16627         93.378636\n",
      "Medical_History_15            13374         75.109514\n",
      "Family_Hist_5                 12540         70.425699\n",
      "Family_Hist_3                 10142         56.958329\n",
      "Family_Hist_2                  8723         48.989105\n",
      "Insurance_History_5            7585         42.598001\n",
      "Family_Hist_4                  5760         32.348647\n",
      "Employment_Info_6              3259         18.302819\n",
      "Medical_History_1              2614         14.680445\n",
      "Employment_Info_4              2022         11.355723\n",
      "Employment_Info_1                10          0.056161\n",
      "\n",
      "\n",
      "For X_train..........Droping Columns with missing values more than 40%.!\n",
      "Total Missing Values Columns List :  ['Medical_History_10', 'Medical_History_32', 'Medical_History_24', 'Medical_History_15', 'Family_Hist_5', 'Family_Hist_3', 'Family_Hist_2', 'Insurance_History_5', 'Family_Hist_4', 'Employment_Info_6', 'Medical_History_1', 'Employment_Info_4', 'Employment_Info_1']\n",
      "Total Number of columns with missing values:  13\n",
      "Number Columns with missing values more than 40%:  8\n",
      "Shape of Data Before droping column:  (41545, 126)\n",
      "Shape of Data After droping column:  (41545, 118)\n",
      "Total Missing Values Columns List After droping the colums:  ['Family_Hist_4', 'Employment_Info_6', 'Medical_History_1', 'Employment_Info_4', 'Employment_Info_1']\n",
      "Total Number of columns with missing values left:  5\n",
      "No. of columns droped:  8\n",
      "For X_train..........Droping Columns Succesfull.!\n",
      "For X_test..........Droping Columns with missing values more than 40%.!\n",
      "Total Missing Values Columns List :  ['Medical_History_10', 'Medical_History_32', 'Medical_History_24', 'Medical_History_15', 'Family_Hist_5', 'Family_Hist_3', 'Family_Hist_2', 'Insurance_History_5', 'Family_Hist_4', 'Employment_Info_6', 'Medical_History_1', 'Employment_Info_4', 'Employment_Info_1']\n",
      "Total Number of columns with missing values:  13\n",
      "Number Columns with missing values more than 40%:  8\n",
      "Shape of Data Before droping column:  (17806, 126)\n",
      "Shape of Data After droping column:  (17806, 118)\n",
      "Total Missing Values Columns List After droping the colums:  ['Family_Hist_4', 'Employment_Info_6', 'Medical_History_1', 'Employment_Info_4', 'Employment_Info_1']\n",
      "Total Number of columns with missing values left:  5\n",
      "No. of columns droped:  8\n",
      "For X_train..........Droping Columns Succesfull.!\n",
      "\n",
      "\n",
      "X_train shape after removing missing value columns:  (41545, 118)\n",
      "X_train shape after removing missing value columns:  (17806, 118)\n",
      "\n",
      "\n",
      "Now Start Imputing the missing value for columns having less than 40% missing values.! \n",
      "After analyzing distributions of columns, we will adopt two strategies to fill missing values: -\n",
      "1.Fill with medain\n",
      "2.Fill with random values of that same column.\n",
      "\n",
      "\n",
      "Missing values Imputation Started.!\n",
      "Imputation in X_train started.!\n",
      "Before impution null values Medical_History_1:  6271\n",
      "Before impution null values Employment_Info_4:  4755\n",
      "Before impution null values Employment_Info_1:  9\n",
      "After impution null values Medical_History_1:  0\n",
      "After impution null values Employment_Info_4:  0\n",
      "After impution null values Employment_Info_1:  0\n",
      "Before impution null values Family_Hist_4     :  13416\n",
      "Before impution null values Employment_Info_6 :  7592\n",
      "After impution null values Family_Hist_4     :  0\n",
      "After impution null values Employment_Info_6 :  0\n",
      "Imputation in X_train completed Succesfully.!\n",
      "\n",
      "Imputation in X_test started.!\n",
      "Before impution null values Medical_History_1:  2614\n",
      "Before impution null values Employment_Info_4:  2022\n",
      "Before impution null values Employment_Info_1:  10\n",
      "After impution null values Medical_History_1:  0\n",
      "After impution null values Employment_Info_4:  0\n",
      "After impution null values Employment_Info_1:  0\n",
      "Before impution null values Family_Hist_4     :  5760\n",
      "Before impution null values Employment_Info_6 :  3259\n",
      "After impution null values Family_Hist_4     :  0\n",
      "After impution null values Employment_Info_6 :  0\n",
      "Imputation in X_test completed Succesfully.!\n",
      "\n",
      "\n",
      "Missing Value details in X_train: \n",
      " Empty DataFrame\n",
      "Columns: [missing_values, missing_values_%]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Missing Value details in X_test: \n",
      " Empty DataFrame\n",
      "Columns: [missing_values, missing_values_%]\n",
      "Index: []\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "X_train shape Before extracting columns from Product_Info_2:  (41545, 118)\n",
      "X_train shape Before extracting columns from Product_Info_2:  (17806, 118)\n",
      "\n",
      "\n",
      "Started Extracting new columns from Product_Info_2 for X_train.!\n",
      "Extraction of new columns from Product_Info_2 for X_train Completed Successfully.!\n",
      "Started Extracting new columns from Product_Info_2 for X_test.!\n",
      "Extraction of new columns from Product_Info_2 for X_test Completed Successfully.!\n",
      "\n",
      "\n",
      "X_train shape after extracting columns from Product_Info_2:  (41545, 119)\n",
      "X_train shape after extracting columns from Product_Info_2:  (17806, 119)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Encoding Ordinal categorical column 'Product_Info_2_alpha' to numerical column using Ordinal Encoding.! \n",
      "Getting the categories.!\n",
      "Categories for the column are:  ['D', 'A', 'E', 'B', 'C']\n",
      "Encoding Started.!\n",
      "Encoding Completed Successfully.!\n",
      "\n",
      "\n",
      "Standardizing Data\n",
      "Converting the data into Scaled values Started.!\n",
      "Converting the data into Scaled values Completed Successfully.!\n",
      "******************************************************************\n",
      "We completed preprocessing the Data.\n",
      "Finally our Data is ready for Training.!!\n",
      "Export the Data to csv so that we can now train our Model.!\n",
      "\n",
      "\n",
      "Saving the files for Training\n",
      "Saving the files for Training Completed Successfully.!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "\n",
    "# loading the data\n",
    "def load_data(path):\n",
    "  print(\"Loading Data.!\")\n",
    "  data = pd.read_csv(path)\n",
    "  print(\"Data succesfully loaded.!\")\n",
    "  return data\n",
    "\n",
    "path=r'Raw_Data\\DATA.csv'\n",
    "data = load_data(path)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# set Id column as index\n",
    "def set_Id_as_index(data):\n",
    "  \"\"\"\n",
    "    This method makes the Id column as index\n",
    "  \"\"\"\n",
    "  print(\"Set ID as Index.!\")\n",
    "  data = data.set_index('Id')\n",
    "  print(\"ID set as index succesfull.!\")\n",
    "  return data\n",
    "\n",
    "print(\"\\n\")\n",
    "data = set_Id_as_index(data)\n",
    "data.head(2)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# remove duplicate rows\n",
    "def remove_duplicated_rows(df):\n",
    "  \"\"\"\n",
    "    This method removes the duplicate rows from the data.\n",
    "  \"\"\"\n",
    "  print(\"Remove Duplicate rows started.!\")\n",
    "  duplicated_values_details = df.duplicated().sum()\n",
    "  print(\"Duplicate Rows present in data: \",duplicated_values_details)\n",
    "  print('Data Shape With Duplicated: ',df.shape)\n",
    "  shape_with_duplicate = df.shape\n",
    "  new_df = df.drop_duplicates()\n",
    "  print('Data Shape Without Duplicated: ',new_df.shape)\n",
    "  shape_without_duplicate = new_df.shape\n",
    "  print(\"No. Duplicated Rows Removed: \",shape_with_duplicate[0]-shape_without_duplicate[0])\n",
    "  print(\"Duplicate Rows in data: \",df.duplicated().sum())\n",
    "  print(\"Duplicate rows removed Succesfully.!\")\n",
    "  return new_df\n",
    "\n",
    "print(\"\\n\")\n",
    "data=remove_duplicated_rows(data)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# splitting the data in independent variables(X) and dependent variables (y)\n",
    "def split_data_into_X_y(data):\n",
    "  \"\"\"This method divides the data into independent variables(X) and dependent variables (y).\n",
    "  -> Takes data frame as input\n",
    "  <- Returns data frames of X, y\n",
    "  \"\"\"\n",
    "  print(\"Spliting the Data into Independent Variable(X),Dependent Variable y started.!\")\n",
    "  X = data.loc[:,data.columns != 'Response']\n",
    "  y= data['Response']\n",
    "  print(\"Data Splited in into Independent Variable(X),Dependent Variable y successfully.!\")\n",
    "  return X,y\n",
    "\n",
    "print(\"\\n\")\n",
    "X,y = split_data_into_X_y(data)\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# split the data for training and testing\n",
    "def train_test_split_X_y(X,y):\n",
    "  \"\"\"This method divides  independent variables(X) and dependent variables (y)\n",
    "     into X_train,X_test,y_train,y_test with 30% data for testing and 70% data for training.\n",
    "     -> Takes data frame X,y as input \n",
    "     <- Returns X_train(data for traing),X_test(data for testing),y_train(data for traing),y_test(data for testing)\n",
    "  \"\"\"\n",
    "  print(\"Train Test Split Started.!\")\n",
    "  X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "  \n",
    "  return X_train,X_test,y_train,y_test\n",
    "\n",
    "print(\"\\n\")\n",
    "X_train,X_test,y_train,y_test= train_test_split_X_y(X,y) \n",
    "\n",
    "print(\"Shape of X_train: \",X_train.shape)\n",
    "print(\"Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of X_test : \",X_test.shape)\n",
    "print(\"Shape of y_test : \",y_test.shape)\n",
    "print(\"Train Test Split Succesfull.!\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\")\n",
    "# checking for missing values in all the data columns\n",
    "def checking_missing_values(df):\n",
    "  \"\"\"This function gives details of missing values and return a dataframe\n",
    "     with two columns, missing values count and the percentage of missing values.\n",
    "     -> Takes original dataframe as parameter\n",
    "     <- Returns dataframe with details of missing values\n",
    "  \"\"\"\n",
    "  missing_values_details = pd.DataFrame(df.isnull().sum(),columns=['missing_values']) \n",
    "  missing_values_details['missing_values_%']=df.isnull().sum()/len(df)*100\n",
    "  missing_values_details = missing_values_details.sort_values(ascending=False,by='missing_values_%')\n",
    "  missing_value_details = missing_values_details[missing_values_details['missing_values']!=0]\n",
    "  return missing_value_details\n",
    "\n",
    "missing_value_details = checking_missing_values(X_train)\n",
    "print(\"Missing Value details in X_train: \\n\",missing_value_details)\n",
    "print(\"\\n\")\n",
    "missing_value_details = checking_missing_values(X_test)\n",
    "print(\"Missing Value details in X_test: \\n\",missing_value_details)\n",
    "print(\"\\n\")\n",
    "\n",
    "# removing columns with more than 50% missing values in all the data columns\n",
    "def remove_missing_value_columns(df,per_missing_value):\n",
    "  \"\"\"This method removes column with more than 50 % missing values.\n",
    "     <- Return data frame with colums removed with more than 50% percent missing values\n",
    "  \"\"\"\n",
    "  missing_value_details = checking_missing_values(df)\n",
    "  missing_values_col_list = missing_value_details[missing_value_details['missing_values_%']>per_missing_value].index.values.tolist()\n",
    "  total_missing_values = missing_value_details[missing_value_details['missing_values_%']!=0].index.values.tolist()\n",
    "  print(\"Total Missing Values Columns List : \",total_missing_values)\n",
    "  print(\"Total Number of columns with missing values: \",len(total_missing_values))\n",
    "  print(f\"Number Columns with missing values more than {per_missing_value}%: \",len(missing_values_col_list))\n",
    "  print(\"Shape of Data Before droping column: \",df.shape)\n",
    "  new_df = df.drop(columns=missing_values_col_list,axis=1)\n",
    "  print(\"Shape of Data After droping column: \",new_df.shape)\n",
    "  \n",
    "  missing_value_details = checking_missing_values(new_df)\n",
    "  #missing_values_col_list = missing_value_details[missing_value_details['missing_values_%']>per_missing_value].index.values.tolist()\n",
    "  total_missing_values = missing_value_details[missing_value_details['missing_values_%']!=0].index.values.tolist()\n",
    "  print(\"Total Missing Values Columns List After droping the colums: \",total_missing_values)\n",
    "  print(\"Total Number of columns with missing values left: \",len(total_missing_values))\n",
    "  print(\"No. of columns droped: \",df.shape[1]-new_df.shape[1])\n",
    "  return new_df\n",
    "\n",
    "print(\"For X_train..........Droping Columns with missing values more than 40%.!\")\n",
    "X_train = remove_missing_value_columns(X_train,40)\n",
    "print(\"For X_train..........Droping Columns Succesfull.!\")\n",
    "print(\"For X_test..........Droping Columns with missing values more than 40%.!\")\n",
    "X_test = remove_missing_value_columns(X_test,40)\n",
    "print(\"For X_train..........Droping Columns Succesfull.!\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"X_train shape after removing missing value columns: \",X_train.shape)\n",
    "print(\"X_train shape after removing missing value columns: \",X_test.shape)\n",
    "print(\"\\n\")\n",
    "print(\"Now Start Imputing the missing value for columns having less than 40% missing values.! \")\n",
    "print(\"After analyzing distributions of columns, we will adopt two strategies to fill missing values: -\")\n",
    "print(\"1.Fill with medain\")\n",
    "print(\"2.Fill with random values of that same column.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"Missing values Imputation Started.!\")\n",
    "\n",
    "\n",
    "# filling missing values with median for columns Medical_History_1,Employment_Info_4,Employment_Info_1\n",
    "def imputing_missing_values(train,test):\n",
    "  imputer1 = SimpleImputer(fill_value=np.nan,strategy='median')\n",
    "  imputer2 = SimpleImputer(fill_value=np.nan,strategy='median')\n",
    "  imputer3 = SimpleImputer(fill_value=np.nan,strategy='median')\n",
    "    \n",
    "\n",
    "  print(\"Imputation in X_train started.!\")\n",
    "  print(\"Before impution null values Medical_History_1: \",train['Medical_History_1'].isnull().sum())\n",
    "  print(\"Before impution null values Employment_Info_4: \",train['Employment_Info_4'].isnull().sum())\n",
    "  print(\"Before impution null values Employment_Info_1: \",train['Employment_Info_1'].isnull().sum())\n",
    "\n",
    "  train['Medical_History_1'] = imputer1.fit_transform(train[['Medical_History_1']])\n",
    "  train['Employment_Info_4'] = imputer2.fit_transform(train[['Employment_Info_4']])\n",
    "  train['Employment_Info_1'] = imputer3.fit_transform(train[['Employment_Info_1']])\n",
    "\n",
    "  print(\"After impution null values Medical_History_1: \",train['Medical_History_1'].isnull().sum())\n",
    "  print(\"After impution null values Employment_Info_4: \",train['Employment_Info_4'].isnull().sum())\n",
    "  print(\"After impution null values Employment_Info_1: \",train['Employment_Info_1'].isnull().sum())\n",
    "\n",
    "\n",
    "  # filling missing values in column Family_Hist_4,Employment_Info_6 with the random-sample-imputation values\n",
    "  print(\"Before impution null values Family_Hist_4     : \",train['Family_Hist_4'].isnull().sum())\n",
    "  print(\"Before impution null values Employment_Info_6 : \",train['Employment_Info_6'].isnull().sum())\n",
    "\n",
    "  train['Family_Hist_4'][train['Family_Hist_4'].isnull()] = train['Family_Hist_4'].dropna().sample(train['Family_Hist_4'].isnull().sum()).values\n",
    "  train['Employment_Info_6'][train['Employment_Info_6'].isnull()] = train['Employment_Info_6'].dropna().sample(train['Employment_Info_6'].isnull().sum()).values\n",
    "\n",
    "  print(\"After impution null values Family_Hist_4     : \",train['Family_Hist_4'].isnull().sum())\n",
    "  print(\"After impution null values Employment_Info_6 : \",train['Employment_Info_6'].isnull().sum())\n",
    "  print(\"Imputation in X_train completed Succesfully.!\")\n",
    "  print(\"\")\n",
    "  # ----------------------------------------------------------------------------------------------------------------\n",
    "  print(\"Imputation in X_test started.!\")\n",
    "  print(\"Before impution null values Medical_History_1: \",test['Medical_History_1'].isnull().sum())\n",
    "  print(\"Before impution null values Employment_Info_4: \",test['Employment_Info_4'].isnull().sum())\n",
    "  print(\"Before impution null values Employment_Info_1: \",test['Employment_Info_1'].isnull().sum())\n",
    "\n",
    "  test['Medical_History_1'] = imputer1.transform(test[['Medical_History_1']])\n",
    "  test['Employment_Info_4'] = imputer2.transform(test[['Employment_Info_4']])\n",
    "  test['Employment_Info_1'] = imputer3.transform(test[['Employment_Info_1']])\n",
    "\n",
    "  print(\"After impution null values Medical_History_1: \",test['Medical_History_1'].isnull().sum())\n",
    "  print(\"After impution null values Employment_Info_4: \",test['Employment_Info_4'].isnull().sum())\n",
    "  print(\"After impution null values Employment_Info_1: \",test['Employment_Info_1'].isnull().sum())\n",
    "\n",
    "\n",
    "  # filling missing values in column Family_Hist_4,Employment_Info_6 with the random-sample-imputation values\n",
    "  print(\"Before impution null values Family_Hist_4     : \",test['Family_Hist_4'].isnull().sum())\n",
    "  print(\"Before impution null values Employment_Info_6 : \",test['Employment_Info_6'].isnull().sum())\n",
    "\n",
    "  test['Family_Hist_4'][test['Family_Hist_4'].isnull()] = test['Family_Hist_4'].dropna().sample(test['Family_Hist_4'].isnull().sum()).values\n",
    "  test['Employment_Info_6'][test['Employment_Info_6'].isnull()] = test['Employment_Info_6'].dropna().sample(test['Employment_Info_6'].isnull().sum()).values\n",
    "\n",
    "  print(\"After impution null values Family_Hist_4     : \",test['Family_Hist_4'].isnull().sum())\n",
    "  print(\"After impution null values Employment_Info_6 : \",test['Employment_Info_6'].isnull().sum())\n",
    "   \n",
    "  print(\"Imputation in X_test completed Succesfully.!\")\n",
    "\n",
    "\n",
    "  return train,test\n",
    "\n",
    "X_train,X_test = imputing_missing_values(X_train,X_test)\n",
    "\n",
    "print(\"\\n\")\n",
    "missing_value_details = checking_missing_values(X_train)\n",
    "print(\"Missing Value details in X_train: \\n\",missing_value_details)\n",
    "print(\"\\n\")\n",
    "missing_value_details = checking_missing_values(X_test)\n",
    "print(\"Missing Value details in X_test: \\n\",missing_value_details)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "# transforming the column Product_Info_2 into two columns.\n",
    "def transform_Product_Info_2(df):\n",
    "  \"\"\"This function converts Product_Info_2 into two columns \n",
    "     Product_Info_2_alpha and Product_Info_2_digit.\n",
    "\n",
    "      Ex - Product_Info_2      Product_Info_2_alpha    Product_Info_2_digit\n",
    "                D3\t                 D                         3\n",
    "                A1                   A                         1\n",
    "     -> Takes original data frame ['Product_Info_2'] as input \n",
    "     <- Returns a new dataframe with two new columns created from Product_Info_2 \n",
    "  \"\"\"\n",
    "  Product_Info_2_alpha=[]\n",
    "  Product_Info_2_digit=[]\n",
    "  for i in df:\n",
    "    Product_Info_2_alpha.append(i[0])\n",
    "    Product_Info_2_digit.append(i[1])\n",
    "  new_df_Product_Info_2 = pd.DataFrame(index = df.index)\n",
    "  new_df_Product_Info_2 = new_df_Product_Info_2.assign(Product_Info_2_alpha=Product_Info_2_alpha,Product_Info_2_digit=Product_Info_2_digit)\n",
    "  return new_df_Product_Info_2\n",
    "\n",
    "\n",
    "# replacing Product_Info_2 with transformed columns feature enginnering.\n",
    "def get_extracted_Product_Info_2(transformed_Product_Info_2,original_df):\n",
    "  \"\"\"This method drops the Product_Info_2 column and \n",
    "     adds the two new columns Product_Info_2_alpha and Product_Info_2_digit.\n",
    "     -> Takes newly transformed datafame which we created from transform_Product_Info_2 function as input and original data frame\n",
    "     <- Returns new data frame for futher processing of project. \n",
    "  \"\"\"\n",
    "  new_transformed_df = pd.concat([transformed_Product_Info_2,original_df],axis=1)\n",
    "  new_transformed_df = new_transformed_df.drop(columns=['Product_Info_2'],axis = 1)\n",
    "  return new_transformed_df\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"X_train shape Before extracting columns from Product_Info_2: \",X_train.shape)\n",
    "print(\"X_train shape Before extracting columns from Product_Info_2: \",X_test.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Started Extracting new columns from Product_Info_2 for X_train.!\")\n",
    "original_df = X_train\n",
    "transform_Product_Info_21 = transform_Product_Info_2(X_train['Product_Info_2'])\n",
    "X_train = get_extracted_Product_Info_2(transform_Product_Info_21,original_df)\n",
    "print(\"Extraction of new columns from Product_Info_2 for X_train Completed Successfully.!\")\n",
    "\n",
    "\n",
    "print(\"Started Extracting new columns from Product_Info_2 for X_test.!\")\n",
    "original_df = X_test\n",
    "transform_Product_Info_21 = transform_Product_Info_2(X_test['Product_Info_2'])\n",
    "X_test = get_extracted_Product_Info_2(transform_Product_Info_21,original_df)\n",
    "print(\"Extraction of new columns from Product_Info_2 for X_test Completed Successfully.!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"X_train shape after extracting columns from Product_Info_2: \",X_train.shape)\n",
    "print(\"X_train shape after extracting columns from Product_Info_2: \",X_test.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# get the categories from ordinal columns\n",
    "def get_Categories(df,column_name):\n",
    "  \"\"\"This method will find the list of all the categories in the column.\n",
    "     -> Takes the data frame and column name \n",
    "     <- Returns the list of categories in the column named.\n",
    "  \"\"\"\n",
    "  categories_list = df[column_name].unique().tolist()\n",
    "  return categories_list\n",
    "\n",
    "\n",
    "# encoding the ordinal categorical columns\n",
    "def encoding_ordinal_column(train,test,list_of_categories,column_name):\n",
    "  \"\"\"This method will encode the Product_Info_2_alpha into numerical column using Ordinal Encoding .\n",
    "     -> Takes train,test data frames, list of categores and column name\n",
    "     <- Returns the train and test data frame encoded.\n",
    "  \"\"\"\n",
    "  oe = OrdinalEncoder(categories=[list_of_categories])\n",
    "  oe.fit(train[[column_name]])\n",
    "  train[column_name] = oe.transform(train[[column_name]])\n",
    "  test[column_name] = oe.transform(test[[column_name]])\n",
    "  return train,test\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Encoding Ordinal categorical column 'Product_Info_2_alpha' to numerical column using Ordinal Encoding.! \")\n",
    "\n",
    "print(\"Getting the categories.!\")\n",
    "categories_list = get_Categories(X_train,'Product_Info_2_alpha')\n",
    "print(\"Categories for the column are: \",categories_list)\n",
    "print(\"Encoding Started.!\")\n",
    "X_train,X_test = encoding_ordinal_column(X_train,X_test,categories_list,'Product_Info_2_alpha')\n",
    "print(\"Encoding Completed Successfully.!\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaling the data\n",
    "def scale_data(train,test):\n",
    "  \"\"\"This method Standardzies the data using StandardScaler\n",
    "     -> Takes trainand test data frames \n",
    "     <- Returns train_scaled and test_scaled data frames.\n",
    "  \"\"\"\n",
    "  sc = StandardScaler()\n",
    "  sc.fit(train)\n",
    "  X_train_scaled = sc.transform(train)\n",
    "  X_test_scaled = sc.transform(test)\n",
    "  X_train_scaled = pd.DataFrame(data=X_train_scaled,columns=train.columns,index = train.index)\n",
    "  X_test_scaled = pd.DataFrame(data=X_test_scaled,columns=test.columns,index = test.index)\n",
    "  return X_train_scaled,X_test_scaled\n",
    "\n",
    "print(\"Standardizing Data\")\n",
    "print(\"Converting the data into Scaled values Started.!\")\n",
    "X_train_scaled,X_test_scaled =  scale_data(X_train,X_test)\n",
    "print(\"Converting the data into Scaled values Completed Successfully.!\")\n",
    "print(\"******************************************************************\")\n",
    "print(\"We completed preprocessing the Data.\")\n",
    "print(\"Finally our Data is ready for Training.!!\")\n",
    "print(\"Export the Data to csv so that we can now train our Model.!\")\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n",
    "def saving_csv_file(X_train,X_test,y_train,y_test):\n",
    "  \"\"\"This method will save all the processed file for Training purpose.\n",
    "  \"\"\" \n",
    "  os.makedirs(r'C:\\Users\\Alkashi\\Desktop\\GL Capstone Project\\Project Folder\\Data_For_Training/',exist_ok=True)\n",
    "  X_train.to_csv(r'C:\\Users\\Alkashi\\Desktop\\GL Capstone Project\\Project Folder\\Data_For_Training/X_train.csv')\n",
    "  X_train.to_csv(r'C:\\Users\\Alkashi\\Desktop\\GL Capstone Project\\Project Folder\\Data_For_Training/X_test.csv')\n",
    "  y_train.to_csv(r'C:\\Users\\Alkashi\\Desktop\\GL Capstone Project\\Project Folder\\Data_For_Training/y_train.csv')\n",
    "  y_test.to_csv(r'C:\\Users\\Alkashi\\Desktop\\GL Capstone Project\\Project Folder\\Data_For_Training/y_test.csv')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Saving the files for Training\")\n",
    "saving_csv_file(X_train,X_test,y_train,y_test)\n",
    "print(\"Saving the files for Training Completed Successfully.!!!!!!!!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0cbaec57d8b5b6d6ff22c80f146358271d5cd2247c78bf1302cc6a79783feb3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
